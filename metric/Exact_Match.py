import token
import tokenize
from io import StringIO
from utils.get_keywords_ops_com_ter import get_keywords_ops_comment

def remove_comment(code_str,language):
    lines=code_str.split("\n")
    _, _, comment,_=get_keywords_ops_comment(language)
    cleaned_line=[]
    for line in lines:
        if line[0:2]!=comment:
            cleaned_line.append(line)
    return cleaned_line


def token_match_score(reference_tokens,hypothesis_tokens,  pad_token="<PAD>"):
    """
    Compute the token-level match score between predicted and reference token sequences.

    Parameters:
        prediction_tokens (List[str]): The list of tokens generated by the model.
        reference_tokens  (List[str]): The ground truth list of tokens to compare against.
        pad_token         (str): The token used to pad the prediction if it's shorter than the reference. Default is "<PAD>".

    Returns:
        float: A token-level accuracy score between 0.0 and 1.0.
               This is the proportion of tokens that exactly match at the same positions.
    """

    ref_len = len(reference_tokens)
    pred_len = len(hypothesis_tokens)

    # Truncate prediction if it's longer than the reference
    if pred_len > ref_len:
        hypothesis_tokens = hypothesis_tokens[:ref_len]
    # Pad prediction if it's shorter than the reference
    elif pred_len < ref_len:
        hypothesis_tokens += [pad_token] * (ref_len - pred_len)

    # Count how many tokens match at each position
    match_count = sum(p == r for p, r in zip(hypothesis_tokens, reference_tokens))

    # Return token-level accuracy
    return match_count / ref_len
